services:
  # OpenAI-compatible server (gratis/local)
  #
  # Dit is optioneel: er zijn veel alternatieven (vLLM OpenAI server, LM Studio, llama.cpp, ...).
  #
  # LocalAI is OpenAI-compatible, maar ge moet w√©l een model configureren.
  # De simpelste workflow is: zet een GGUF model in `./local-llm/models/` en voeg een LocalAI config toe.
  # (CPU kan, maar is traag; GPU is aanbevolen.)
  localai:
    image: localai/localai:latest-aio-cpu
    ports:
      # Matcht `.env.example`: OPENAI_BASE_URL=http://localhost:8000/v1
      - "8000:8080"
    environment:
      - MODELS_PATH=/models
    volumes:
      - ./local-llm/models:/models
