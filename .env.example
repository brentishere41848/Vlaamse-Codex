# Plat Vlaamsâ€‘Only AI (lokaal / self-hosted)
#
# Kopieer naar `.env.local` en pas aan.

# OpenAI-compatible endpoint (gratis/local)
# Lokaal: bv. LocalAI/LM Studio/llama.cpp OpenAI server op uw eigen machine
OPENAI_BASE_URL=http://localhost:8000/v1
OPENAI_MODEL=local-model
# mag leeg zijn voor veel lokale servers; anders: zet uw eigen key
OPENAI_API_KEY=

# (optioneel) model gedrag
OPENAI_TEMPERATURE=0.3
OPENAI_TIMEOUT_S=20

# VlaamsCodex AI webserver (serveert `website/` + /api/chat)
VLAAMSCODEX_AI_HOST=127.0.0.1
VLAAMSCODEX_AI_PORT=5174
VLAAMSCODEX_WEB_ROOT=website

# Vercel:
# - `/api/chat` draait als serverless function (zie `api/chat.py`)
# - `OPENAI_BASE_URL` mag dan NIET `localhost` zijn (Vercel kan niet naar uw PC).
#   Zet dat naar uw eigen publiek bereikbare OpenAI-compatible server.

# Hardening (zonder auth)
AI_RATE_LIMIT_PER_MINUTE=30
AI_MAX_INPUT_CHARS=8000
